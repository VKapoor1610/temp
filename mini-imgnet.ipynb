{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data.dataloader import Dataset , DataLoader\n",
    "\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets \n",
    "\n",
    "import torch.autograd as tag\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting, device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(\" Starting, device =\" ,device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'n01968897': 0, 'n01770081': 1, 'n01818515': 2, 'n02011460': 3, 'n01496331': 4, 'n01847000': 5, 'n01687978': 6, 'n01740131': 7, 'n01537544': 8, 'n01491361': 9, 'n02007558': 10, 'n01735189': 11, 'n01630670': 12, 'n01440764': 13, 'n01819313': 14, 'n02002556': 15, 'n01667778': 16, 'n01755581': 17, 'n01924916': 18, 'n01751748': 19, 'n01984695': 20, 'n01729977': 21, 'n01614925': 22, 'n01608432': 23, 'n01443537': 24, 'n01770393': 25, 'n01855672': 26, 'n01560419': 27, 'n01592084': 28, 'n01914609': 29, 'n01582220': 30, 'n01667114': 31, 'n01985128': 32, 'n01820546': 33, 'n01773797': 34, 'n02006656': 35, 'n01986214': 36, 'n01484850': 37, 'n01749939': 38, 'n01828970': 39, 'n02018795': 40, 'n01695060': 41, 'n01729322': 42, 'n01677366': 43, 'n01734418': 44, 'n01843383': 45, 'n01806143': 46, 'n01773549': 47, 'n01775062': 48, 'n01728572': 49, 'n01601694': 50, 'n01978287': 51, 'n01930112': 52, 'n01739381': 53, 'n01883070': 54, 'n01774384': 55, 'n02037110': 56, 'n01795545': 57, 'n02027492': 58, 'n01531178': 59, 'n01944390': 60, 'n01494475': 61, 'n01632458': 62, 'n01698640': 63, 'n01675722': 64, 'n01877812': 65, 'n01622779': 66, 'n01910747': 67, 'n01860187': 68, 'n01796340': 69, 'n01833805': 70, 'n01685808': 71, 'n01756291': 72, 'n01514859': 73, 'n01753488': 74, 'n02058221': 75, 'n01632777': 76, 'n01644900': 77, 'n02018207': 78, 'n01664065': 79, 'n02028035': 80, 'n02012849': 81, 'n01776313': 82, 'n02077923': 83, 'n01774750': 84, 'n01742172': 85, 'n01943899': 86, 'n01798484': 87, 'n02051845': 88, 'n01824575': 89, 'n02013706': 90, 'n01955084': 91, 'n01773157': 92, 'n01665541': 93, 'n01498041': 94, 'n01978455': 95, 'n01693334': 96, 'n01950731': 97, 'n01829413': 98, 'n01514668': 99}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainLoader( Dataset):\n",
    "    \n",
    "    def __init__(self , data_dir , transform = None ):\n",
    "        super().__init__() \n",
    "        self.data_dir = data_dir \n",
    "        self.transform = transform\n",
    "        \n",
    "        with open( os.path.join( data_dir , 'Labels.json')) as f : \n",
    "            self.class_labels = json.load(f) \n",
    "        \n",
    "        # Dataloader iterable is stored in images , labels lists \n",
    "        self.images : list = [] \n",
    "        self.labels : list = [] \n",
    "        \n",
    "        \n",
    "        for i in range( 1 , 2 ):  # from train.X1 to train.X4 \n",
    "            \n",
    "            xi_dir = os.path.join( self.data_dir , f'train.X{i}')\n",
    "            \n",
    "            for classname in os.listdir( xi_dir )  : \n",
    "                \n",
    "                class_dir = os.path.join( xi_dir , classname)\n",
    "                label_name = self.class_labels[ classname]\n",
    "                \n",
    "                for image_name  in os.listdir( class_dir):\n",
    "                    \n",
    "                    image_path = os.path.join( class_dir , image_name)\n",
    "                    \n",
    "                    \n",
    "                    self.images.append( image_path )\n",
    "                    self.labels.append( dic[classname] )\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len( self.labels)\n",
    "    \n",
    "    def __getitem__(self , idx ) :\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.open( image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform : \n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image , label\n",
    "\n",
    "class CustomTestLoader(Dataset):\n",
    "    \n",
    "    def __init__(self , data_dir , transform= None):\n",
    "        super().__init__() \n",
    "        self.root_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open( \"./archive/Labels.json\") as f:\n",
    "            self.class_labels = json.load(f) \n",
    "        \n",
    "        self.images :list = [] \n",
    "        self.labels :list = [] \n",
    "        \n",
    "        for class_name in os.listdir( os.path.join(self.root_dir , 'val.X')):\n",
    "            \n",
    "            class_dir = os.path.join( self.root_dir , class_name)\n",
    "            \n",
    "            label = self.class_labels[class_name]\n",
    "            \n",
    "            for image_name in os.listdir( class_dir):\n",
    "                \n",
    "                image_path = os.path.join( class_dir , image_name)\n",
    "                \n",
    "                self.images.append( image_path)\n",
    "                self.labels.append(label)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx ):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        return image , label \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizeFunction(tag.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, qp=0.5, normalize=True):\n",
    "        ctx.qp = qp\n",
    "        ctx.normalize = normalize\n",
    "        if normalize == True:\n",
    "            output = qp*torch.round(tensor/qp)\n",
    "        else:\n",
    "            output = torch.round(tensor/qp)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.normalize == True:\n",
    "            grad_input = grad_output.clone()\n",
    "        else:\n",
    "            grad_input = grad_output.clone()/ctx.qp\n",
    "        return grad_input, None, None\n",
    "\n",
    "class QuantizeLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Custom defined.\n",
    "    \"\"\"\n",
    "    def __init__(self, qp=0.5, normalize=True):\n",
    "        super(QuantizeLayer, self).__init__()\n",
    "        self.qp = qp\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(\"Quantizer used \")\n",
    "        return QuantizeFunction.apply(input, self.qp, self.normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, dataloader):\n",
    "  net.to(device)\n",
    "  net.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "      for batch in dataloader:\n",
    "          images, labels = batch[0].to(device), batch[1].to(device)\n",
    "          outputs = net(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "  return correct/total\n",
    "\n",
    "\n",
    "def smooth(x, size):\n",
    "  return np.convolve(x, np.ones(size)/size, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Train(net, dataloader, testLoader=None ,  epochs=1, start_epoch=0, lr=0.01, momentum=0.9, decay=0.0005,\n",
    "          verbose=1, print_every=100, state=None, schedule={}, checkpoint_path=None):\n",
    "\n",
    "  net.to(device)\n",
    "  net.train()\n",
    "\n",
    "# To store the losses for plotting purpose\n",
    "  losses = []\n",
    "\n",
    "    \n",
    "  task_criterion = nn.CrossEntropyLoss()  # 100 class classification ( 1-hot-encoding )\n",
    "\n",
    "# Stochastic Gradient Descend\n",
    "  optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
    "\n",
    "  # Load previous training state (Not Required for this model )\n",
    "  if state:\n",
    "      net.load_state_dict(state['net'])\n",
    "      optimizer.load_state_dict(state['optimizer'])\n",
    "      start_epoch = state['epoch']\n",
    "      losses = state['losses']\n",
    "\n",
    "  # Fast forward lr schedule through already trained epochs\n",
    "  for epoch in range(start_epoch):\n",
    "    if epoch in schedule:\n",
    "      print (\"Learning rate: %f\"% schedule[epoch])\n",
    "      for g in optimizer.param_groups:\n",
    "        g['lr'] = schedule[epoch]\n",
    "\n",
    "# TRAINING START\n",
    "  for epoch in range(start_epoch, epochs):\n",
    "    sum_loss = 0.0\n",
    "\n",
    "    # Update learning rate when scheduled\n",
    "    if epoch in schedule:\n",
    "      print (\"Learning rate: %f\"% schedule[epoch])\n",
    "      for g in optimizer.param_groups:\n",
    "        g['lr'] = schedule[epoch]\n",
    "\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "       \n",
    "        inputs, labels = batch[0].to(device), (torch.Tensor(batch[1])).to(device)\n",
    "\n",
    "        optimizer.zero_grad() #1\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # TASK LOSS\n",
    "        # Approach: Classification using categorical cross entropy.\n",
    "        task_loss = task_criterion(outputs, labels)\n",
    "\n",
    "        task_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        losses.append(task_loss.item())\n",
    "        sum_loss += task_loss.item()\n",
    "\n",
    "        if i % print_every == print_every-1:    # print every 10 mini-batches\n",
    "            if verbose:\n",
    "              print('[%d, %5d] loss: %.5f' % (epoch, i + 1, sum_loss / print_every))\n",
    "            sum_loss = 0.0\n",
    "\n",
    "    print(\"EPOCH \", epoch)\n",
    "    print(\" Training Accuracy is: \" , accuracy(net ,  dataloader) , end= \"|\")\n",
    "    \n",
    "    # print( \"Testing Accuracy is: \", accuracy(net , testLoader), end = \"\\n\\n\")\n",
    "\n",
    "    if checkpoint_path: #(Not required for this case)\n",
    "      state = {'epoch': epoch+1, 'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'losses': losses}\n",
    "      torch.save(state, checkpoint_path + 'checkpoint-%d.pkl'%(epoch+1))\n",
    "\n",
    "  return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "\n",
    "def getData( root_dir = './archive/'  ):\n",
    "    \n",
    "    transform_train = transforms.Compose(\n",
    "        [ transforms.Resize( (128 , 128 )) ,  transforms.ToTensor() , transforms.Normalize( mean = (0.5 , 0.5 , 0.5) , std = (0.5 , 0.5 , 0.5)) , transforms.RandomHorizontalFlip( p=0.20) ]\n",
    "    )\n",
    "    transform_test = transforms.Compose(\n",
    "        [transforms.Resize((128 , 128 )) , transforms.ToTensor() , transforms.Normalize( mean = (0.5 , 0.5 , 0.5 ) , std = (0.5 , 0.5 , 0.5))]\n",
    "    )\n",
    "    \n",
    "    train_data = CustomTrainLoader( data_dir= root_dir  , transform = transform_train)\n",
    "    # test_data = CustomTestLoader( data_dir= root_dir , transform= transform_test )\n",
    "    \n",
    "    train_loader = DataLoader( train_data , batch_size = 128 , shuffle = True )\n",
    "    # test_loader = DataLoader( test_data , batch_size = 128 , shuffle= False )\n",
    "    \n",
    "    # train_loader = DataLoader( CustomTrainLoader( data_dir = root_dir , transform = transform )  , batch_size = 128 , shuffle = True )\n",
    "    # test_loader = DataLoader( CustomTestLoader(data_dir = root_dir , transform = transform) , batch_size = 128 , shuffle = False )\n",
    "    # return 0 \n",
    "    return {'train': train_loader , 'test': None }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self , qp= None , embed_dim = 200 ):\n",
    "        super().__init__()\n",
    "        self.qp = qp\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, stride=2, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.Conv2d(32, 64, stride=2, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.Conv2d(64, 64, stride=2, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.Conv2d(64, 64, stride=2, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # The Latent Vector dimension is set as 200 can be reduced as per needs.\n",
    "        self.z_mean = torch.nn.Linear(2048*2, self.embed_dim)\n",
    "        self.z_log_var = torch.nn.Linear(2048*2, self.embed_dim)\n",
    "\n",
    "        if qp is not None: \n",
    "            self.quantize = QuantizeLayer(qp = qp , normalize= True )\n",
    "        else: \n",
    "            self.quantize = None\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                torch.nn.Linear(self.embed_dim, 2048*2),\n",
    "                Reshape(-1, 64, 8, 8),\n",
    "                #\n",
    "                nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                nn.Conv2d(64, 64, stride=1, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                nn.Conv2d(64, 64, stride=1, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                nn.Conv2d(64, 32, stride=1, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                nn.Conv2d(32, 3, stride=1, kernel_size=3, padding=1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "\n",
    "    def encoding_fn(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.quantize(self.z_mean(x)), self.quantize(self.z_log_var(x))\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "    def reparameterize(self, z_mu, z_log_var):\n",
    "        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        if( self.quantize == None):\n",
    "            z_mean, z_log_var = self.z_mean(x),self.z_log_var(x)\n",
    "            \n",
    "        else:\n",
    "            z_mean, z_log_var = self.quantize(self.z_mean(x)), self.quantize(self.z_log_var(x))\n",
    "            \n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, z_mean, z_log_var, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getData(root_dir='./archive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-> resnet 18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/vihan/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/storage/vihan/anaconda3/envs/gpu_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/storage/vihan/anaconda3/envs/gpu_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# resnet_model = torchvision.models.resnet18(pretrianed = True)\n",
    "\n",
    "resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No of out_features = 100 \n",
    "# print(resnet_model)\n",
    "resnet_model.fc = nn.Linear( in_features= resnet_model.fc.in_features , out_features=100 , bias = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VAE( qp = None , embed_dim = 200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combined_model( nn.Module ):\n",
    "    \n",
    "    def __init__( self , resnet , vae ):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.resnet = resnet \n",
    "        self.vae = vae \n",
    "    \n",
    "    def forward( self , x ):\n",
    "        return self.resnet( self.vae(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = combined_model( resnet_model , vae_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.010000\n",
      "EPOCH  0\n",
      " Training Accuracy is:  0.9571446063425982|EPOCH  1\n",
      " Training Accuracy is:  0.0|EPOCH  2\n",
      " Training Accuracy is:  0.0|Learning rate: 0.001000\n",
      "EPOCH  3\n",
      " Training Accuracy is:  0.0|EPOCH  4\n",
      " Training Accuracy is:  0.0|EPOCH  5\n",
      " Training Accuracy is:  0.0|EPOCH  6\n",
      " Training Accuracy is:  0.0|"
     ]
    }
   ],
   "source": [
    "losses= Train( resnet_model , data['train'] , epochs = 7 , schedule= {0:0.01 , 3:0.001 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['train'].__len__() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 128, 128])\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate( data['train'] , 0 ):\n",
    "    # print( type( batch[0]))\n",
    "    # print(  type( batch[1]))\n",
    "    print( batch[0].shape )\n",
    "    print( len(batch[1]))\n",
    "    \n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
